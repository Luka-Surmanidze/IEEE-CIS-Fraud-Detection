# IEEE-CIS-Fraud-Detection



კონკურსის მოკლე მიმოხილვა:


კონკურსი, რომელზეც ამ დავალბის შესრულების დროს უნდა გვემუშავა, არის Fraud detection ტიპის, რაც გულისხმობს იმას რომ, მოცემული გვაქვს დატასეტი, მსგავსი საბანკო ტრანზაქციებისა,
ჩვენ კი გვევალება სწორად გამოვიცნოთ, რომელი ტრანზაქცია არის Fraud ტიპის.


ჩემი მიდგომა პრობლემის გადასაჭრელად:

როგორც პირველი დავალების შემთხვევაში, აქაც თავიდან დავიწყე დატასეტის გაანალიზება, ვეცადე ბოლომდე გამომერიცხა Data Leakage-ის შანსები, ვნახე რამდენი რამდენი ცვლადი მქონდა, 
რამდენი კატეგორიული სვეტი იყო დატასეტში, შესაბამისად დავიწყე ფიქრი რა მოდელები შემეძლო გამომეყენებინა, და რადგან კლასიფიკაციის ამოცანაა, და თან ამ დავალებაში საინტერესო იყო 
მოდელებს შორის სხვაობა, შესაბამისად დავიწყე ერთი შეხედვით ყველაზე პრიმიტიული მოდელის დატრენინგებით, (იგულისხმება Logistic Regression), 
და ბოლო მიზნად მქონდა ისეთი კომპლექსური მოდელების გატესტვა როგორიც არის RandomForest და XGBoost.



რეპოზიტორიის სტრუქტურა:

model-experiment-{model_name} - კონკრეტული მოდელის კოდი
model-inference - დასაბმითების კოდი.
README - რიდმი.


Feature Engineering:

სანამ უშუაოლოდ ინჟინერიის ნაწილზე გადავალთ, მონაცემების გაანალიზების დროს წავაწყდი Data Leakage ის საფრთხეს, რომელზეც ირიბად კონკურსის აღწერაშიც იყო მინიშნება, 
ვგულისხმობ ერთი და იმავე პიროვნების ტრანზაქციების ტრეინში და ტესტ სეტებში მოხვედრას. ეს გამოიწვევდა Leakage-ს, რადგან შესაძლოა გვქონდეს ამ პიროვნების 5 ტრანზაქცია 
და მეხუთე ტრანზაქციის პრედიქშენი იყოს 1 იანი. თუ ეს ერთიანი არ იქნება ტრეინში დანარჩენ ტრანზაქციებთან ერთად, მოდელი ვერ ისწავლის რა იყო ის ცვლილება რამაც გამოიწვია 
პრედიქშენი შეცვლილიყო 0დან 1ზე, და დიდი ალბათობით, ასეტი ტიპის ტესტზე არასწორად დააბრუნებს 0 იანს. შესაცბამისად, რაღაც მახასიათებლით
უნდა დაგვეჯგუფებინა ეს ტრანზაქციები, და გამოგვეყო identitiy ცვლადი. დატაზე დაკვირვების შედეგად, ორი მიდგომა გამოიკვეთა, ერთი identitiy-ის ცვლადებით გაკეთება 
ამ ყველაფრის, ან ტრანზაქციებში უკვე არსებულ ცვლადებით.საუკეთესო შედეგი აჩვნა ტრანზაქციების დატის: card1, addr1, da P_emaildomain ის კონკატენაციამ 
და ასეთნაირად identitiy ცვლადის გამოყვანამ. ამასთან შევამოწმე ვარიაციაც დაყოფილ სეტებში მოხვედრილი ერთიანობის რაოდენობის მიმართ, და ჩემი ცდებიდან, ამ ცდას ყველაზე 
ნაკლები ჰქონდა, შესაბამისად, საკმაოდ თანაბრად გადანაწილდა. დანარჩენი მსჯელობა დამყარებული იქნება დატასეტის ასეთნაირად გადანაწილებაზე. დატაზე მუშაობის წინ გამოვყავი
X_train(60%), X_val(20%) და X_test(20%) დატასეტები. 

გარდა ამისა მეორე მნიშვნელოვანი პრობლემა იყო დაუბალანსებელი დატა(დაახლოებით 1-33 თან ფარდობა), შესაბამისად საჭირო გახდა UnderSampling და OverSampling 
ტექნიკების გამოყენება.


ინჟინერიის პირველი ნაწილი არის cleaning. რადგან პირველი ვაპირებდი გამეტესტა ლოჯისტიკური რეგრესია, დავიწყე ფიქრი როგორ მომეშორებინა nan ები. 
რადგან საკმაოდ ბევრი ცვლადი გვქონდა დატაში, გონივრული იქნებოდა რომ ისეთი ცვლადები რომლის უმრავლესობაც nan ია გადაგვეყარა, თუ უმრავლესობა nan ია თავის თავში 
ინფორმაციას არ ატარებს ესეიგი ამ ცვლადის nan ობა, და თან განზომილებასაც შევამცირებთ.შესაბამისად, მოვსინჯე პროცენტების რამოდენიმე theshold. 
დანარჩენი nan ები კი უბრალოდ mode-ით შევავსე. თუმცა მეორე ვერსიის ქლინინგში, გადავწყვიტე ისეთ ცვლადებზე რომლებზეც ინფორმაცია შედარებით მქონდა,
ანუ ვიცოდი დაახლოებით რას აღნიშნავდა, და თან მნიშვნელოვანი იყო, როგორიც არის მისამართი, იმეილი და ასე შემდეგ, შემევსო ისეთი მნიშვნელობით, რომელიც
საკმაოდ გადახრილი იქნებოდა განაწილებისგან და მოდელს მიანიშნებდა, რომ ამ ცვლადზე უბრალოდ ინფორმაცია არ გვაქვს. ეს წესით იმუშავებდა, მაგრამ ჩემს შემთხვევაში
დიდი გაუმჯობესება არ ჰქონია, მეორე ვარიანტია, რომ ისევ is_uknown ცვლადები გამოუყვანო ფიჩერებზე, მაგრამ ზუსტად იგივე იდეას ატარებს ეგ და შესაბამისად აღარ მიცდია.

ამ ნაბიჯების შემდეგ განზომილება საკმაოდ შემცირდა, დაახლოებით 70-ით. ამის შემდეგ დარჩენილი კატეგორიული ცვლადების რიცხვითში გადაყვანა დავიწყე.
წინა დავალების მსგავსად გამოვიყენე one_hot_encoding თუმცა რადგან არ მინდოდა ბევრი ცვლადი დამემატებინა, 3-ზე მეტი უნიკალური ცვლადის შემთხვევაში 
გამოვიყენე woe_encoding. 

რადგან ცვლადების უმრავლესობა არ იყო ცნობილი, და თან საკმაოდ ბევრი ცვლადი მქონდა ისედაც, ბევრი ახალი ცვლადი ჩემით არ გამომიყვანია, მხოლოდ identity, და ასევე 
ვცადე ერთი შეხედვით მნიშვნელოვანი ცვლადებიდანახალი ცვლადების გამოყვანა, როგორიცაა card ის მონაცემები.


Feature Selection:
სელექშენის ნაწილი განსხვავდება მოდელების მიხედვით, რადგან ისეთი მოდელები რომლებიც გადაწყვეტილების ხეების სტრუქტურას ემყარება, დიდად არ საჭიროებს ცვლადების გადარჩევას,
რადგან თვითონ აგვარებს ამას. თუმცა  გადავარჩიე ვარიანტები მაინც. პირველი კორელაციებს ვპოულობდი, და მჭიდროდ კორელირებული ცვლადებიდან ვშლიდი იმ მათგანს რომელიც ნაკლებად
კორელირებული იყო target თან. ასევე თითოეული ცვლადის მნიშვნელობების განაწილების მიხედვით გამოვიყვანე მათი ვარიაცები და დავაკვირდი ტენდენციას. მხვდებოდა ცვლადები რომლებსაც ძალიან დიდი
ვარიაცია ჰქონადა, მაგრამ ასევე ბევრი იყო ისეთიც რომლის ვარიაცია 0თან ახლოს იყო. ჩავთვალე დაბალ ვარიაციანი ცვლადები გადამეყარა, რადგან ეს დიდად ინფორმაციას არ მომცემდა, თუმცა როგორც 
აღმოჩნდა ეს არც თუ ისე სწორი ნაბიჯი იყო, ეს კი ვარიანტების გადარჩევის დროს გამოჩნდა. მერე მიხვდი, რომ რადგან მონაცემები არის დაუბალანსებელი, ვგულისხმობ იმას რომ 97% არის 0, დანარჩენი
კი 1, ამ დაბალ ვარიაციან ცვლადებში, ძალიან იშვიათი ცვლილება იწვევდა სწორედ, 0 ის 1 ად გადაქცევას. შესაბამისად რაც უფრო მცირდებოდა threshold უკეთესი შედეგი ჰქონდა. თუმცა 0 სა და
0.01 ს შორის არ იყო დიდი სხვაობა, ამიტომაც მეტი განზოგადებისთვის შევარჩიე ეს threshold. ამ ყველაფრის შემდეგ სადღაც 170 ცვლადი რჩება, რაც არ არის ნამდვილად ბევრი, თუმცა
რეგრესიის მოდელისთვის ტრეინის შემდეგ მაინც ვცადე შაპის ველიუების დათვლა feature importance ისთვის, ეს ასევე დამეხმარა მიმხვდარიყავი ვარიაციის მიხედვით გაფილტვრა რაღაც მხრივ რატომ იყო 
არასწორი, რადგან შევამჩნიე ყველაზე მნიშვნელოვანი ცვლადები მიქრებოდა. ტენდენციაას რომ დავაკვირდი ვნახე რომ საბოლოო ჯამში ტოპ 150 ცვლადის დატოვება იქნებოდა ოპტიმალური.


Training:

Logistic Regression:
ლოჯისტიკურ რეგრესიისთვის და მითუმეტეს როცა ამდენი ცვლადი გაქვს, მნიშვნელოვანია სქეილინგი. მითუემეტეს რომ გვქონდა რამოდენიმე ცვლადი,(მაგ: TransactionDT) რომლთა საშუალო და
ვარიაცია ძალიან განსხვავებოდა სხვებისგან, რაც სავარაუდოა რომ გამოიწვევდა რადიკალურ ცვლილებებს ლოსის დათვლისას, შესაბამისად ეს რომ არ მომხდარიყო გამოვიყენეთ სტანდარტული სქეილერი.
გარდა ამისა გადასაჭრელი იყო არაბალანსირებული მონაცემების პრობლემა, ამისთვის ვცადე RandomOverSampler და RandomUnderSampler, ეს ორი მათგანი passthrough სთან ერთად
გადავარჩიე, საბოლოო ჯამში, ლოჯისტიკური რეგრესიისთვის ოპტიმალური იყო 0.2 იანი UnderSampler. ასევე class_weight იც გამოვიყენე და ეს ცვლადიც გადავერჩიე, 
შესაძლო იყო ამას მოეგვარებინა ეგ პრობლემა, თუმცა საუკეთესო შედეგი ვალიდაციაზე, None ს ანიჭებს class_weight ს, რადგან sampler: UnderSampler და class_weight:None იყო
საუკეთესო მოდელის წყვილი პარამეტრები. penalty აირჩა l2, ხოლო რეგულარიზციის კოეფიციენტი C ცვლადი გადავარჩიე [0.01, 1] დიაპაზონში, საუკეთესო შედეგი ჰქონდა 0.1 ს.

შედეგი ტრეინზე: 0.8522

შედეგი ვალიდაციაზე: 0.8437

შედეგი ტესტზე: 0.8241

საკმაოდ კარგი შედეგია რეგრესიისთვის, თავიდან ვფიქრობდი რომ ამაზე უარესი შედეგი ექნებოდა, მაგრამ თუ ჩავთვლით რომ შეცდომა არ დაგვიშვია cleaning ის დროს, და სწორი ცვლადები
დავტოვეთ, საკმაოდ დიდი ნაწილი ცვლადებისა სწორედ წრფივ კავშირშია თარგეთთან, რისი ვიზუალიზაციაც გავაკეთე პლოტის დახმარებით და იქაც დადასტრუდა ჩემი ვარაუდი. ცხადია არის რაღაც
ნაწილი ცვლადებისა რომელიც არ ერგება წრფივ მოდელს და შესაბამისად ესაა მიზეზი იმის რომ ყველაზე უარესი მოდელია იმ ოთხ მოდელს შორის რომელიც განვიხილე. ბაიასი ნორმალურია, არ არის
დიდი სხვაობა ტრეინსა და ტესტს შორის, ვარიაციაც მისაღებია, მოკლედ რომ ვთქვა ის არის რასაც ველოდებოდი.

Decision Tree:

ამ მოდელის შემთხვევაში სქეილინგი აღარ გამიკეთებია, რადგან გადაწყვეტილების ხისთვის ამას არ აქვს მნიშვნელობა. პრეპროცესინგის ნაწილი მეტწილად იგივე არის. sampler ისევ გამოვიყენე ცხადია,
რადგან ბალანსირებულობის პრობლემა განურჩევლად მოდელისა ყველგან არის. ასევე რადგან Decision Tree ს ერთ-ერთი მთავარი პრობლემა არის ის რომ მიდის overfit ში, ამის გამო ვცადე
max_depth(საუკეთესო 12), min_samples_leaf(საუკეთესო 25) და min_samples_split(საუკეთესო 5) ჰიპერმარამეტრებით ეს ყოველივე გამეკონტროლებინა. საბოლოო ჯამში მივიღე ასეთი შედეგები:

შედეგი ტრეინზე: 0.8822

შედეგი ვალიდაციაზე: 0.8454

შედეგი ტესტზე: 0.8386

აქ მნიშვნელოვანია ტრეინის სქორს დავაკვირდეთ, რაც ვახსენე რომ გადაწყვეტილების ხეს აქვს ოვერფიტის პრობლემა აქ გამოჩნდა, რადგან ბაიასი გვიმცირდება, მაგრამ ვარიაცია გვეზრდება, რაც ტრეინისა
და ტესტის სქორებზე დაკვირვებისას ჩანს. ასევე min_samples_leaf პარამეტრი სხვა ხის სტრუქტურაზე დაფუძნებულ მოდელებთან შედარებით საკმაოდ დიდია, რაც ისევ იმ აზრს ამყარებს, რომ საკმაოდ
დიდი დოზით რეგულარიზაცია სჭირდება. საბოლოო ჯამში ცოტათი მოგვცა გაუმჯობესება. ოპტიმალურ ხის სიღრმედ აირჩა 12, ამ პარამეტრის ნებისმიერი ერთეულით ზრდა იწვევდა ტრეინის სქორის
გაზრდას და ტესტის სქორის შემცირებას პარალელურად, ანუ უკვე იზეპირებდა დატასეტს. 


RandomForest:

პრეპროცესინგის ნაწილი ზუსტად იგივე რაც წინა მოდელში იყო. ჰიპერმარამეტრებიც იგივეა, ოღონდ დაემატა n_estimator ჰიპერმარამეტრი. ეს ჰიპერმარამეტრი დაგვეხმარება, მოვაგვაროთ ის პრობლემა
რაც გვქონდა წინა მოდელის შემთხვევაში, შევამციროთ ვარიაცია, რაც ბევრი ესტიმატორის(ხის) მიერ მიღებული პასუხის გასაშუალოების (ამ შემთხვევაში მაქსიმალურის) არჩევით ხდება. 
min_samples_leaf პარამეტრმა დაიკლო საგრძნობლად(2 ზე ჩამოვიდა), რაც იმის მანიშნებელია რომ ბევრი ხის ხარჯზე აღარ ხდება ოვერფიტ ტრეინზე, და არ საჭიროებს ფოთლების შეზრუდვას.
თუმცა სიღმრე გაიზარდა, ესეც ლოგიკურია რადგან, ამ შემთხვევაში ჩვენი მოდელი სწავლობს ბევრ ხეს და თან დეტალურად, ცალკეულ ხეზე გვექნება ოვერფიტი, მაგრამ ბევრი ხის საშუალო შედეგი,
აუცილებლად იქნება ყველაზე ახლოს პასუხთან.

შედეგი ტრეინზე: 0.94

შედეგი ვალიდაციაზე: 0.89

შედეგი ტესტზე: 0.87

როგორც ვხედავთ ტრეინს გაცილებით უკეთ სწავლობს, და ტესტზეც უკეთესი შედეგი აქვს. მაგრამ ვარიაცია მაინც გვაქვს ტესტსა და ტრეინს შორის. ეს ხდება იმიტომ რომ უფრო მეტი
რეგულარიზაცია არის საჭირო, და თან იმ 200 ხის აგება მაინც ტრეინზე ხდება ამიტომ ესეც ლოგიკურია.

XGBoost:

პრეპროცესინგის ნაწილში ქლინინგი გრიდსერჩით გადავარჩიე, ერთ შემთხვევაში საერთოდ არ ვიღებ ფიჩერებს, მეორე შემთხვევაში კი ვიყენებ იგივე ქლინინგის მეთოდს, რომელსაც
წინა მოდელებში ვიყენებდი. მოდელს ბოლომდე მივეცი საშუალება რომ ჰქონოდა ბევრი ცვლადის გადარჩევის შესაძლებლობა, და ამან გაამართლა კიდეც რადგან, საუკეთესო შედეგი სწორედ
passthrough ს შემთხვევაში ჰქონდა მოდელს ვალიდაციაზე. 
ჰიპერმარამეტრებიც იგივეა თითქმის ოღონდ ემატება რეგულარიზაციისთვის საჭირო ჰიპერპარამეტრები, როგორიცაა learning rate(საუკეთესო 0.1), min_child_weight(საუკეთესო 1) და ასე შემდეგ.
min_child_weight ის ველიუს თუ დავაკვირდებით არც თუ ისე მკაცრი რეგულარიზაციაა თუმცა ამას ემატება learning_rate იც, ამ ორივეს ხარჯზე კი აუთლაიერებს ვაგვარებთ.

შედეგი ტრეინზე: 0.96

შედეგი ვალიდაციაზე: 0.90

შედეგი ტესტზე: 0.89

აქ სამივე პარამეტრი უმჯობესდება, თუმცა ტრეინზე ბაიასი მაინც ძალიან დაბალია. მიუხედავად ამისა კარგია რომ, ვალიდაცია და ტესტი ახლოს არის ერთმანეთთან.

საბოლოო ჯამში XGBoost აღმოჩნდა საუკეთესო მოდელი.
დასაბმითების შემდეგ  0.90 დაწერა სქორი kaggle-მა, რასთანაც ძალიან ახლოს არის ჩემი შეფასება ტესტზე. ესეიგი ტესტიდანაც არაფერი დაგვილიქავს ისე
დავატრენინგეთ მოდელი, რამაც ნამდვილად გაასწორა.

